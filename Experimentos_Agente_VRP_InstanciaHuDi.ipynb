{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn1_7h8Tjo8d"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "tempdir = tempfile.gettempdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-gEtHxvYLte"
      },
      "source": [
        "#**Random Fuzzy Number Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzTDtDrNRv10"
      },
      "outputs": [],
      "source": [
        "#!pip install portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzeNvftxUFJP"
      },
      "outputs": [],
      "source": [
        "#import portion as P\n",
        "from random import random as rm\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCyq2YfeWRYW"
      },
      "source": [
        "# Alpha-cut based (for unimodal Fuzzy Numbers)\n",
        "From \"Fuzzy Uncertainty in Random Variable Generation: An a-Cut Approach\", by Christian Alfredo Varón-Gaviria, José Luis Barbosa-Fontecha, and Juan Carlos Figueroa-García"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6MRYdwCYMts"
      },
      "outputs": [],
      "source": [
        "def build_triangular_mf(a, b, c):\n",
        "  if a > b or b > c:\n",
        "    raise ValueError(\"Parameters must form a nondecreasing sequence\")\n",
        "  return lambda x: (x-a)/(b-a) if a <= x < b else (c - x)/(c - b) if b <= x < c else 0\n",
        "\n",
        "def build_sampleable_triangular_mf(a, b, c):\n",
        "  if a > b or b > c:\n",
        "    raise ValueError(\"Parameters must form a nondecreasing sequence\")\n",
        "  Lambda1 = (b - a) / 2\n",
        "  Lambda2 = (c - b) / 2\n",
        "  Lambda = (c - a) / 2\n",
        "\n",
        "  lambda1 = Lambda1 / Lambda\n",
        "  lambda2 = Lambda2 / Lambda\n",
        "\n",
        "  return lambda u1, u2: u1 * (b - a) + a if u2 <= lambda1 else c - u1 * (c - b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEiWmaXvTLDj"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "scrTF-PoTP9b",
        "outputId": "fe5c9f22-7261-424a-fb71-ff02b793f146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff502d2c150>]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "n = 100\n",
        "\n",
        "sampler = build_sampleable_triangular_mf(23, 45, 62)\n",
        "mf = build_triangular_mf(23, 45, 62)\n",
        "x = [sampler(rm(), rm()) for i in range(n)]\n",
        "y = [mf(u) for u in x]\n",
        "plt.plot(x, y, 'o', color='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "selsh4D2X8oU"
      },
      "source": [
        "# Cumulative membership function-based\n",
        "From \"Fuzzy Uncertainty in Random Variable Generation: A Cumulative Membership\n",
        "Function Approach\", by Diana Giseth Pulido-L´opez, Mabel García, and Juan Carlos Figueroa-García"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NATqXt_kYMT0"
      },
      "outputs": [],
      "source": [
        "def build_trapezoidal_mf(a, b, c, d):\n",
        "  if a > b or b > c or c > d:\n",
        "    raise ValueError(\"Parameters must form a nondecreasing sequence\")\n",
        "  return lambda x: (x-a)/(b-a) if a <= x < b else 1 if b <= x < c else (d - x)/(d - c) if c <= x < d else 0\n",
        "\n",
        "def build_sampleable_trapezoidal_mf(a, b, c, d):\n",
        "  if a > b or b > c or c > d:\n",
        "    raise ValueError(\"Parameters must form a nondecreasing sequence\")\n",
        "  Lambda1 = (b - a) / 2\n",
        "  Lambda2 = c - b\n",
        "  Lambda3 = (d - c) / 2\n",
        "  Lambda = (c + d - a - b) / 2\n",
        "\n",
        "  lambda1 = Lambda1 / Lambda\n",
        "  lambda2 = Lambda2 / Lambda\n",
        "  lambda3 = Lambda3 / Lambda\n",
        "\n",
        "  return lambda u: a + math.sqrt((b - a) * 2 * u * Lambda) if u <= lambda1 else u * Lambda + b - Lambda1 if u <= (lambda1 + lambda2) else d - math.sqrt((d - c)**2 - 2 * (d - c) * (u * Lambda - Lambda1 - Lambda2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4tOlCfIFGri",
        "outputId": "f6700875-2c24-45f5-955c-c5f6608edb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(58.948035711210224, 1)\n",
            "(29.910895894054242, 0.3141316315479201)\n",
            "(67.291787306826, 0.8530059081437223)\n",
            "(33.04360879630299, 0.4565276725592268)\n",
            "(83.48196632379026, 0.4032787132280483)\n",
            "(37.83709669385343, 0.6744134860842469)\n",
            "(42.857409144504786, 0.9026095065683993)\n",
            "(59.66462482321444, 1)\n",
            "(47.10716989101542, 1)\n",
            "(50.24441930645341, 1)\n",
            "(57.055655740095716, 1)\n",
            "(36.34383189669101, 0.6065378134859549)\n",
            "(80.89993965161568, 0.4750016763440088)\n",
            "(59.64738501283911, 1)\n",
            "(62.520570087315384, 0.9855397197967949)\n",
            "(46.31019201040909, 1)\n",
            "(70.99822690006549, 0.7500492527759586)\n",
            "(65.09973084296342, 0.9138963654732382)\n",
            "(69.58835678228205, 0.7892123116032764)\n",
            "(38.862964174308374, 0.7210438261049261)\n",
            "(67.6457874163782, 0.8431725717672722)\n",
            "(69.30874687853708, 0.79697925337397)\n",
            "(84.0915394790308, 0.3863461255824778)\n",
            "(35.246540807406916, 0.5566609457912235)\n",
            "(62.13355960081341, 0.9962900110885163)\n",
            "(43.955023544598575, 0.9525010702090261)\n",
            "(40.36443416319827, 0.7892924619635576)\n",
            "(55.64868585532291, 1)\n",
            "(51.081205592210196, 1)\n",
            "(48.91669208590386, 1)\n",
            "(35.09102509246483, 0.5495920496574924)\n",
            "(55.303345866343605, 1)\n",
            "(71.187995679777, 0.7447778977839721)\n",
            "(70.5909782207811, 0.7613617160894139)\n",
            "(33.73558673948264, 0.4879812154310292)\n",
            "(50.57448988147354, 1)\n",
            "(78.67902977598857, 0.536693617333651)\n",
            "(72.9096376195131, 0.6969545105690804)\n",
            "(45.4048988951514, 1)\n",
            "(54.68787386218264, 1)\n",
            "(66.51669238781366, 0.8745363225607315)\n",
            "(63.72994799835453, 0.9519458889345963)\n",
            "(58.027531890088454, 1)\n",
            "(45.73775557364105, 1)\n",
            "(69.88825028734878, 0.7808819364625338)\n",
            "(35.104943166791855, 0.5502246893996298)\n",
            "(82.86853330909037, 0.42031851919193425)\n",
            "(45.749136883515064, 1)\n",
            "(45.63480712460691, 1)\n",
            "(48.29870763975076, 1)\n",
            "(77.41687343881253, 0.5717535155885408)\n",
            "(56.06694819517108, 1)\n",
            "(53.253717331022926, 1)\n",
            "(47.570655786332736, 1)\n",
            "(58.91879058421233, 1)\n",
            "(48.13589470310272, 1)\n",
            "(49.75245000622687, 1)\n",
            "(36.42543719191148, 0.6102471450868854)\n",
            "(75.70822085835026, 0.6192160872680483)\n",
            "(58.554976690741384, 1)\n",
            "(86.56113157438215, 0.31774634515605155)\n",
            "(65.16197278607498, 0.9121674226090283)\n",
            "(57.298956884826126, 1)\n",
            "(65.77339561790599, 0.895183455058167)\n",
            "(50.7465886951486, 1)\n",
            "(59.58945747539359, 1)\n",
            "(68.506901414443, 0.8192527384876942)\n",
            "(75.66871904523417, 0.6203133598546064)\n",
            "(50.709760791258816, 1)\n",
            "(64.43893133931373, 0.9322519072412854)\n",
            "(54.19571505732779, 1)\n",
            "(75.98333234237182, 0.6115741016007826)\n",
            "(82.51355004038633, 0.43017916554482405)\n",
            "(55.758985335133914, 1)\n",
            "(44.00868468016344, 0.9549402127347019)\n",
            "(41.36097033603307, 0.8345895607287759)\n",
            "(64.6658533590784, 0.9259485178033778)\n",
            "(72.15216325386633, 0.7179954651703796)\n",
            "(60.713345719006895, 1)\n",
            "(44.372394283547436, 0.9714724674339744)\n",
            "(53.208881018701206, 1)\n",
            "(38.149951251777374, 0.6886341478080624)\n",
            "(51.67030453229911, 1)\n",
            "(63.09019925362321, 0.9697166873993552)\n",
            "(78.6618512280568, 0.5371707992206444)\n",
            "(48.240158754199726, 1)\n",
            "(48.99512646738212, 1)\n",
            "(39.052919902115875, 0.7296781773689034)\n",
            "(51.47472515645034, 1)\n",
            "(62.5097767759563, 0.9858395340012139)\n",
            "(69.90782586000141, 0.7803381705555164)\n",
            "(70.11923181113454, 0.7744657830240406)\n",
            "(40.54448880119477, 0.7974767636906712)\n",
            "(83.82828578279694, 0.3936587282556406)\n",
            "(56.028678440115385, 1)\n",
            "(58.487440571314096, 1)\n",
            "(77.67759844764717, 0.564511154232023)\n",
            "(46.119649695988954, 1)\n",
            "(56.32448245697492, 1)\n",
            "(46.47061749001404, 1)\n"
          ]
        }
      ],
      "source": [
        "sampler = build_sampleable_trapezoidal_mf(23, 45, 62, 98)\n",
        "mf = build_trapezoidal_mf(23, 45, 62, 98)\n",
        "\n",
        "for j in range(100):\n",
        "  r = sampler(rm())\n",
        "  print(\"({}, {})\".format(r, mf(r)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tox7BrKIGiAy"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "TDCx2C08G45k",
        "outputId": "6476fb19-dc01-4cce-9e64-9aa14c97d2c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff50281ecd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "n = 1000\n",
        "x = [sampler(rm()) for i in range(n)]\n",
        "y = [mf(u) for u in x]\n",
        "plt.plot(x, y, 'o', color='black')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwtc5bvkalfJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhLtb1vx9bHq"
      },
      "source": [
        "#**Simulación de una instancia de VRP utilizando Simpy**\n",
        "\n",
        "Características: \n",
        "\n",
        "*   1 vehiculo con límite de capacidad.\n",
        "*   1 depósito (inicio y fin del recorrido).\n",
        "*   N clientes.\n",
        "*   Se posee una estimación de las demandas de los clientes y del tiempo de recorrido entre cada uno de ellos incluído el depósito.\n",
        "*   La simulación finaliza al regresar al depósito.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFHci8LJ_FiN"
      },
      "source": [
        "## Instalación e importación de librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi0Fpsao7yPF",
        "outputId": "55a0a516-89f9-47fd-c9cb-0b6a367648ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpy in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install simpy\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkRFtzNmumlz"
      },
      "outputs": [],
      "source": [
        "import simpy\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO7jx1Mj_Rah"
      },
      "source": [
        "## Definicion e inicializacion de estructuras de datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TstuMh2-dTyv"
      },
      "source": [
        "### Samplers for Random Fuzzy Number Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYOmbunc7BR"
      },
      "outputs": [],
      "source": [
        "##\n",
        "sampler_triangular = build_sampleable_triangular_mf(0.3, 0.7, 2.0) \n",
        "##\n",
        "#sampler_trapezoidal_prod_estim = build_sampleable_trapezoidal_mf(23, 35, 43, 55)\n",
        "#sampler_trapezoidal_prod_pedido = build_sampleable_trapezoidal_mf(23, 35, 43, 55)\n",
        "sampler_trapezoidal_prod_pedido = build_sampleable_trapezoidal_mf(30, 45, 55, 75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysWVdcMAv8CB",
        "outputId": "0a49a384-1e3b-4b1a-aded-b03f505399df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4592217290870883"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#sampler_trapezoidal_prod_estim(rm())\n",
        "sampler_triangular(rm(),rm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq-7hQbNz_OZ"
      },
      "outputs": [],
      "source": [
        "# Tabla de distancias/tiempos estimados entre los clientes, incluyendo el deposito (intervalo fuzzy)\n",
        "# revisar esta tabala para generar un intervalo fuzzy !!!\n",
        "carnicerias = ['Frigorifico Tres Arroyos','La Blanq y D Gaspar','La Blanquita','Rougier Carnes','La Pulperia','Granja de Rosso','La Estancia','El Buen Trato','El Ternero Gasolero','La Imbatible']\n",
        "tiempos = np.array([[0, 9, 6, 4, 4, 7, 9, 10, 12, 10],\n",
        "[8, 0, 5, 8, 8, 9, 9, 10, 11, 9],\n",
        "[6, 6, 0, 5, 5, 5, 5, 8, 7, 5],\n",
        "[4, 8, 5, 0, 1, 4, 5, 5, 7, 6],\n",
        "[5, 9, 6, 3, 0, 5, 7, 6, 8, 7],\n",
        "[7, 9, 6, 5, 5, 0, 4, 3, 5, 4],\n",
        "[8, 9, 5, 6, 5, 2, 0, 5, 4, 2],\n",
        "[9, 12, 9, 7, 6, 5, 6, 0, 5, 7],\n",
        "[10, 11, 7, 8, 7, 5, 4, 3, 0, 4],\n",
        "[9, 9, 6, 7, 6, 5, 3, 7, 6, 0]])\n",
        "df_tiempos = pd.DataFrame(tiempos, columns=carnicerias, index=carnicerias)\n",
        "df_tiempos\n",
        "\n",
        "# Tabla de Planificacion inicial de orden de visita, con estimacion de cantidad de productos requeridos (intervalo fuzzy)\n",
        "# revisar esta tabala para generar un intervalo fuzzy !!!\n",
        "data_planificacion = np.array([['La Blanq y D Gaspar',1,4, 40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['La Blanquita',2,3, 40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['Rougier Carnes',3,2,40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['La Pulperia',4,1, 40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['Granja de Rosso',5,5,40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['La Estancia',6,6,40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['El Buen Trato',7,9,40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['El Ternero Gasolero',8,8,40,int(sampler_trapezoidal_prod_pedido(rm())),0],\n",
        "                              ['La Imbatible',9,7,40,int(sampler_trapezoidal_prod_pedido(rm())),0]])\n",
        "df_planificacion =  pd.DataFrame(data_planificacion, columns=['cliente','idCliente','orden','productosEstimados','productosPedidos','productosRecibidos'])\n",
        "df_planificacion\n",
        "\n",
        "# Tabla de \"registro de viaje\": orden de visita real, confirmacion de atencion, duracion del viaje, tiempos de llegada, tiempo de atencion, cantidad de pedido real (en el momento), cantidad de productos realmente entregados\n",
        "#data_viaje = np.array([0,0,0,0,0,0,0,0]) # se cargaria durante la simulacion\n",
        "df_viaje =  pd.DataFrame( columns=['Cliente','orden_visita_real','duracion_viaje','tiempo_llegada','duracion_espera','posponer','tiempo_atencion','pedido','duracion_atencion','entregar','entregado'])\n",
        "df_viaje\n",
        "\n",
        "# Vehiculos (capacidad, prod disp, prod entregados, tiempos/dist recorrida... )\n",
        "capacity_vehicle_1 = 450\n",
        "available_products_vehicle_1 = 410\n",
        "warehouse_vehicle_1 = 'Frigorifico Tres Arroyos' #ubicacion inicial\n",
        "current_time=''\n",
        "current_event= ''\n",
        "current_location = warehouse_vehicle_1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iEjPAP_dCr"
      },
      "source": [
        "## Implementación de la clase 'Vehicle'\n",
        "\n",
        "Esta clase define e implementa la lógica de la simulación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2X8dvnq2P3v"
      },
      "outputs": [],
      "source": [
        "class Vehicle(object):\n",
        "    def __init__(self, env, capacity, available_products, times_map, planning, trip_log, warehouse):\n",
        "        self.env = env\n",
        "        #self.vehicle = simpy.Resource(env, num_vehicles)\n",
        "        self.available_products = available_products\n",
        "        self.travel_time = 0\n",
        "        self.times_map = times_map.copy()\n",
        "        self.planning = planning.sort_values('idCliente') #.set_index('orden')\n",
        "        self.trip_log = trip_log\n",
        "        self.clients_inorder = planning.sort_values('orden')[['cliente','orden']].set_index('orden')\n",
        "        self.warehouse = warehouse\n",
        "        self.current_location = warehouse\n",
        "        self.customers_visited = 0\n",
        "        self.current_event= ''\n",
        "        self.current_time=''\n",
        "        self.current_customerId = None\n",
        "        self.cancel = 0\n",
        "        self.next_customer = 1\n",
        "        self.max_viajes = len(self.planning.index) * 2\n",
        "        self.times_map_slow = self.times_map.copy()\n",
        "        self.times_map_slow[['Frigorifico Tres Arroyos','La Blanquita','La Pulperia','Granja de Rosso','El Ternero Gasolero']] = 4* self.times_map_slow[['Frigorifico Tres Arroyos','La Blanquita','La Pulperia','Granja de Rosso','El Ternero Gasolero']]\n",
        "        self.times_map_slow[['La Blanq y D Gaspar','Rougier Carnes','La Estancia','El Buen Trato','La Imbatible']] = 1.5 * self.times_map_slow[['La Blanq y D Gaspar','Rougier Carnes','La Estancia','El Buen Trato','La Imbatible']]\n",
        "\n",
        "    def set_next_customer(self, id_customer): \n",
        "        self.next_customer = id_customer\n",
        "\n",
        "    def run (self,env):      \n",
        "        initial_time = env.now\n",
        "        self.current_event = 0 #'inicio'\n",
        "\n",
        "        # print('Inicio de run')\n",
        "\n",
        "        #realizar el recorrido visitando los clientes (mientras el próximo viaje no sea volver al depósito)\n",
        "        trip_number=0\n",
        "        while (self.next_customer!=0 and trip_number<self.max_viajes):\n",
        "          self.cancel = 0\n",
        "          trip_number += 1\n",
        "          self.current_customerId = self.next_customer\n",
        "          customer= self.planning.set_index('idCliente').at[str(self.current_customerId),\"cliente\"]\n",
        "\n",
        "          yield self.env.process(self.travel_next_customer(customer))\n",
        "\n",
        "          #En esta version no se considera la espera\n",
        "          #yield self.env.process(self.wait_customer(customer))\n",
        "          if (self.cancel == 0):\n",
        "            yield self.env.process(self.check_order(customer))\n",
        "            yield self.env.process(self.deliver_products(customer))\n",
        "          total_travel_time = env.now - initial_time\n",
        "          #yield self.env.timeout(1)\n",
        "          \n",
        "        #volver al deposito\n",
        "        trip_duration = self.times_map.loc[self.current_location].loc[self.warehouse] #tiempo de viaje desde la ubicacion actual al deposito\n",
        "        yield self.env.timeout(trip_duration)\n",
        "        self.current_event = 6 #'fin'\n",
        "        total_travel_time = self.env.now - initial_time\n",
        "\n",
        "\n",
        "    def travel_next_customer(self, customer): \n",
        "        initial_travel_time = self.env.now\n",
        "\n",
        "        #print('Viajando a %s' % customer)\n",
        "\n",
        "        # Generar el tiempo de viaje desde la ubicacion actual al proximo cliente\n",
        "        if (self.env.now<120):\n",
        "          travel_time = self.times_map.loc[self.current_location].loc[customer] #tiempo de viaje desde la ubicacion actual al proximo cliente\n",
        "        else:\n",
        "          travel_time = self.times_map_slow.loc[self.current_location].loc[customer] #tiempo de viaje desde la ubicacion actual al proximo cliente\n",
        "\n",
        "        #yield self.env.timeout(random.randint(1, 3)) \n",
        "        yield self.env.timeout(travel_time) \n",
        "        \n",
        "        #print('Arribo a %s' % customer)\n",
        "       \n",
        "        self.customers_visited = self.customers_visited + 1\n",
        "        self.current_event = 1 # ('travel-'+str(self.customers_visited+1))\n",
        "\n",
        "        # Geenrar estimacion de espera / En esta version no se considera la espera\n",
        "        #wait_estimated = random.uniform(1,5)  # Revisar distribucion\n",
        "        wait_estimated = 0\n",
        "\n",
        "        # Crear el registro en trip_log\n",
        "        #'Cliente','orden_visita_real','duracion_viaje','tiempo_llegada','duracion_espera','posponer','tiempo_atencion','pedido','duracion_atencion','entregar','entregado'\n",
        "        self.trip_log.loc[self.customers_visited] = (customer, self.customers_visited, travel_time, self.env.now, wait_estimated,0,0,0,0,0,0)\n",
        "\n",
        "        #Actualizar la ubicacion y la hora actual\n",
        "        current_location = customer\n",
        "        self.current_time= self.env.now\n",
        "\n",
        "    def wait_customer(self, customer):   # ver como modificarlo para que pueda haber una consulta/decision\n",
        "        if (self.trip_log.at[self.customers_visited, 'posponer'] == 0):         \n",
        "          #print('Esperando en %s' % customer)\n",
        "          yield self.env.timeout(self.trip_log.at[self.customers_visited, 'duracion_espera'])  \n",
        "          self.current_event = 2 #'wait-'+str(self.customers_visited)        \n",
        "        else:\n",
        "          # se saltea el cliente por lo que hay que modificar la planificacion para volver luego\n",
        "          #print('Espera cancelada de %s' % customer)\n",
        "          self.cancel == 1\n",
        "          self.current_event = 3 #'salteado-'+str(self.customers_visited)\n",
        "\n",
        "          #actualizar la planificacion - POR EL MOMENTO SE ENVIA AL CLIENTE AL FINAL DE LA COLA\n",
        "          nuevaVisita = self.planning.set_index('orden').iloc[self.customers_visited]\n",
        "          self.planning=self.planning.append(nuevaVisita,ignore_index=True)\n",
        "          self.planning.at[self.planning.shape[0]-1, 'orden'] = self.planning.shape[0]\n",
        "\n",
        "        self.current_time= self.env.now\n",
        "\n",
        "    def check_order(self, customer): \n",
        "        #print('Consultando pedido en %s' % customer)\n",
        "\n",
        "        pedido_inicial = int(self.planning.set_index('idCliente').at[str(self.current_customerId),\"productosPedidos\"])\n",
        "        recibido_actual = int(self.planning.set_index('idCliente').at[str(self.current_customerId),\"productosRecibidos\"])\n",
        "        pedir = pedido_inicial - recibido_actual\n",
        "        \n",
        "        self.trip_log.at[self.customers_visited, 'pedido'] = pedir\n",
        "\n",
        "        yield self.env.timeout(1/60)\n",
        "\n",
        "        self.current_event = 4 #'check_order-'+str(self.customers_visited)\n",
        "\n",
        "        self.current_time= self.env.now\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def deliver_products(self, customer):  \n",
        "        # Entregar productos, registrar y actualizar variables\n",
        "        #print('Despachando productos en %s' % customer)\n",
        "\n",
        "        entregar = int(self.trip_log.at[self.customers_visited, 'entregar'])\n",
        "        if (entregar>self.available_products):\n",
        "          entregar = self.available_products\n",
        "\n",
        "        deliver_time = entregar * sampler_triangular(rm(),rm())\n",
        "\n",
        "        self.available_products = self.available_products - entregar\n",
        "        self.trip_log.at[self.customers_visited, 'duracion_atencion'] = deliver_time\n",
        "        self.trip_log.at[self.customers_visited, 'entregado'] = entregar\n",
        "\n",
        "        recibido_ant = int(self.planning.set_index('idCliente').at[str(self.current_customerId),\"productosRecibidos\"])\n",
        "        #print('recibido_ant: ',recibido_ant)\n",
        "        #print('entregar: ',entregar)\n",
        "        self.planning.at[self.current_customerId,\"productosRecibidos\"] = recibido_ant + entregar\n",
        "        #print('recibido_ant + entregar: ',recibido_ant + entregar)\n",
        "\n",
        "        yield self.env.timeout(abs(deliver_time))\n",
        "\n",
        "        self.current_event = 5  #'deliver_products-'+str(self.customers_visited)\n",
        "        self.current_time= self.env.now\n",
        "\n",
        "        #print('Productos disponibles: ')\n",
        "        #print(self.available_products)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_o1AfBQR7D6"
      },
      "source": [
        "______________________________\n",
        "# **Ensamble en un Python Environment de TF-Agents**\n",
        "\n",
        "Ensamble de la simulación de la instancia VRP en un Python Environment de TF-Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGcwDaQD8lb"
      },
      "source": [
        "## Instalación e importación de librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeGv4GHrR7XA",
        "outputId": "8c3c2afe-9edc-4b72-a8da-a720e434606e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (2.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.0.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.2.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.5.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --upgrade pip\n",
        "#!pip install --upgrade tensorflow\n",
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSLU_7yiSMi2"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "#import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "#from tf_agents.environments import wrappers\n",
        "#from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "JNcDfMNufCov",
        "outputId": "8e279ba2-fe2f-4790-f559-3a8ea871d159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Cliente orden_visita_real duracion_viaje tiempo_llegada  \\\n",
              "1  La Blanq y D Gaspar                 1              9              9   \n",
              "\n",
              "  duracion_espera posponer tiempo_atencion pedido duracion_atencion entregar  \\\n",
              "1               0        0               0      0                 0        0   \n",
              "\n",
              "  entregado  \n",
              "1         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-234185ff-517e-4ad3-ac25-7537b1dec823\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cliente</th>\n",
              "      <th>orden_visita_real</th>\n",
              "      <th>duracion_viaje</th>\n",
              "      <th>tiempo_llegada</th>\n",
              "      <th>duracion_espera</th>\n",
              "      <th>posponer</th>\n",
              "      <th>tiempo_atencion</th>\n",
              "      <th>pedido</th>\n",
              "      <th>duracion_atencion</th>\n",
              "      <th>entregar</th>\n",
              "      <th>entregado</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>La Blanq y D Gaspar</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-234185ff-517e-4ad3-ac25-7537b1dec823')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-234185ff-517e-4ad3-ac25-7537b1dec823 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-234185ff-517e-4ad3-ac25-7537b1dec823');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df_viaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWmq3aX7EH5R"
      },
      "source": [
        "\n",
        "## Implementación de la clase 'TSPEnv'\n",
        "\n",
        "Esta clase implementa un entorno para aprendizaje por refuerzo utilizando la API de PyEnvironment de la librería TF-Agents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo0xVyeYSaPF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TSPEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, planning, estimated_time, reward_discount=1.1):\n",
        "\n",
        "    self.planning = planning\n",
        "    self.estimated_time = estimated_time\n",
        "    self.reward_discount = reward_discount\n",
        "    self.customers = len(self.planning.index)\n",
        "\n",
        "    #Componentes de la acción/decisión:\n",
        "    #1- Define la cantidad de productos a entregar a un cliente. Opciones: 0:no entregar; 1:entregar solo lo estimado;  2:entregar lo solicitado\n",
        "    #2- Define el proximo destino\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(2,), dtype=np.int32, minimum=[0,0], maximum=[2,self.customers], name='action')\n",
        "    \n",
        "    #Datos que debería ver el agente en cada paso:\n",
        "    #-#1- Evento:\n",
        "        #0: Inicio del recorrido\n",
        "        #1: llegada al cliente\n",
        "        #2: termina espera y es atendido\n",
        "        #3: cliente salteado (indica cómo reorganiza??)\n",
        "        #4: pedido registrado\n",
        "        #5: entrega realizada\n",
        "        #6: llegada a deposito. FIN\n",
        "    #2- Número del Cliente actual\n",
        "    #3- Tiempo actual\n",
        "    #4- Mercadería disponible\n",
        "    #5- Pedido del cliente (si corresponde, sino 0)\n",
        "    #-#6- Cantidad entregada (si corresponde, sino 0)\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(4,), dtype=np.float32, minimum=0, name='observation')\n",
        "    \n",
        "    self.env = simpy.Environment()\n",
        "    self.vehicle = Vehicle(self.env, capacity_vehicle_1, available_products_vehicle_1, df_tiempos, self.planning, df_viaje, warehouse_vehicle_1)\n",
        "    \n",
        "    #self._state = np.array([0,0,0,self.vehicle.available_products,0,0])\n",
        "    self._state = [0,0,self.vehicle.available_products,0]\n",
        "    self._episode_ended = False\n",
        "    self.old_event = self.event = self.new_event= None\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "\n",
        "  def reward_spec(self): \n",
        "    \"\"\"Defines the rewards that are returned by `step()`.\n",
        "    Override this method to define an environment that uses non-standard reward\n",
        "    values, for example an environment with array-valued rewards.\n",
        "    Returns:\n",
        "      An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n",
        "    \"\"\"\n",
        "    return array_spec.ArraySpec(shape=(), dtype=np.float32, name='reward')\n",
        "\n",
        "\n",
        "  def time_step_spec(self) -> ts.TimeStep:\n",
        "    \"\"\"Describes the `TimeStep` fields returned by `step()`.\n",
        "    Override this method to define an environment that uses non-standard values\n",
        "    for any of the items returned by `step()`. For example, an environment with\n",
        "    array-valued rewards.\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing (possibly nested) `ArraySpec`s defining\n",
        "      the step_type, reward, discount, and observation structure.\n",
        "    \"\"\"\n",
        "    return ts.time_step_spec(self.observation_spec(), self.reward_spec())\n",
        "\n",
        "\n",
        "  def _reset(self):\n",
        "    # Resetea la simulacion y ejecuta el paso inicial\n",
        "    self.env = simpy.Environment()\n",
        "    self.vehicle = Vehicle(self.env, capacity_vehicle_1, available_products_vehicle_1, df_tiempos, self.planning, df_viaje, warehouse_vehicle_1)\n",
        "    self.env.process(self.vehicle.run(self.env))\n",
        "    self.env.step()\n",
        "    self.event = self.new_event = self.vehicle.current_event\n",
        "\n",
        "    # Actualiza el estado y devuelve la observacion inicial\n",
        "    #self._state = np.array([0,0,0,self.vehicle.available_products,0,0])\n",
        "    self._state = [0,0,self.vehicle.available_products,0]\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32),reward_spec=array_spec.ArraySpec(shape=(), dtype=np.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _step(self, action):\n",
        "\n",
        "    action = tf.cast(action, tf.int32)\n",
        "    action = action.numpy()\n",
        "    self.old_event = self.event\n",
        "    current_customer = self.vehicle.current_customerId\n",
        "    next_customer = action[1]\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    # Make sure episodes don't go on forever.\n",
        "    if self.vehicle.current_event == 6:\n",
        "      self._episode_ended = True\n",
        "\n",
        "    else:\n",
        "      #print('acciones: ',action[0],'  ',action[1],' evento actual: ',self.vehicle.current_event)\n",
        "      if self.vehicle.current_event == 4: # DEFINIR CANTIDAD A ENTREGAR\n",
        "        if (action[0] == 0): #NADA\n",
        "          entregar = 0\n",
        "        elif (action[0] == 1): # solo la mitad de lo que pide  ##OLD:  SOLO LO ESTIMADO\n",
        "          #copy_planing = self.vehicle.planning.copy()\n",
        "          #entregar = copy_planing.sort_values('idCliente').at[self.vehicle.current_customerId-1, 'productosEstimados']\n",
        "          entregar = int(self.vehicle.trip_log.at[self.vehicle.customers_visited, 'pedido'])\n",
        "          entregar = entregar/2\n",
        "        else: # TODO LO QUE PIDIÓ\n",
        "          entregar = int(self.vehicle.trip_log.at[self.vehicle.customers_visited, 'pedido'])\n",
        "        self.vehicle.trip_log.at[self.vehicle.customers_visited, 'entregar'] = entregar\n",
        "\n",
        "      elif (self.vehicle.current_event == 5 or self.vehicle.current_event == 0):\n",
        "        #Definir a que cliente viaja o si termina el recorrido\n",
        "        self.vehicle.set_next_customer(action[1])\n",
        "\n",
        "      #AVANZAR sim HASTA PROXIMO EVENTO\n",
        "      until = 1000000\n",
        "      while (self.event == self.new_event or self.new_event == 1) and (self.env.peek() < until): # \"env.peek() < until\" controla que no haya llegado al fin la ejecucion (peek da el tiempo del proximo evento)\n",
        "           self.env.step()\n",
        "           self.new_event= self.vehicle.current_event\n",
        "      self.event = self.new_event\n",
        "\n",
        "      #ACTUALIZAR EL ESTADO EL ENTORNO  [ Evento, Número del Cliente actual, Tiempo actual, Tiempo de espera estimado , Pedido del cliente, entregado ]\n",
        "      #self._state = np.array([self.vehicle.current_event,self.vehicle.current_customerId,self.vehicle.current_time,self.vehicle.available_products,self.vehicle.trip_log.at[self.vehicle.customers_visited, 'pedido'],self.vehicle.trip_log.at[self.vehicle.customers_visited, 'entregado']])\n",
        "      #self._state = [self.vehicle.current_event,self.vehicle.current_customerId,self.vehicle.current_time,self.vehicle.available_products,self.vehicle.trip_log.at[self.vehicle.customers_visited, 'pedido'],self.vehicle.trip_log.at[self.vehicle.customers_visited, 'entregado']]\n",
        "      self._state = [self.vehicle.current_customerId,self.vehicle.current_time,self.vehicle.available_products,self.vehicle.trip_log.at[self.vehicle.customers_visited, 'pedido']]\n",
        "      #print(self._state)\n",
        "\n",
        "    # Ver si esto es necesario mas adelante\n",
        "    #else:\n",
        "    #  raise ValueError('`action` should be 0 or 1.')\n",
        "\n",
        "    if self._episode_ended:\n",
        "\n",
        "      pedido = self.vehicle.trip_log['pedido'].to_numpy(dtype=int)\n",
        "      entregado = self.vehicle.trip_log['entregado'].to_numpy(dtype=int)\n",
        "      \n",
        "      satisfaccion = sum(pedido-entregado)\n",
        "      satisfaccion = -(satisfaccion**2)\n",
        "\n",
        "      recompensa = satisfaccion * (self.reward_discount**(self.vehicle.current_time/60)) #revisar formula\n",
        "\n",
        "      # sumar penalizacion por clientes no visitados\n",
        "      recibidos = self.vehicle.planning['productosRecibidos'].to_numpy(dtype=int)\n",
        "      clientes_sin_entrega = recibidos.size - np.count_nonzero(recibidos)\n",
        "      clientes_sin_entrega = clientes_sin_entrega * (-40)\n",
        "      recompensa = recompensa + clientes_sin_entrega\n",
        "\n",
        "      recompensa = (sum(entregado) + clientes_sin_entrega)\n",
        "\n",
        "      #Descuento o aumento por tiempo transcurrido\n",
        "      if (recompensa>0): recompensa = recompensa * (0.9**(self.vehicle.current_time/60)) \n",
        "      else: recompensa = recompensa * (1.1**(self.vehicle.current_time/60)) \n",
        "      \n",
        "      reward = np.array(recompensa,dtype=np.float32) \n",
        "\n",
        "      #return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "      return ts.TimeStep(step_type= ts.StepType.LAST, reward=reward,  discount=np.array(0.0,dtype=np.float32),  observation=np.array(self._state, dtype=np.float32))\n",
        "      else:\n",
        "      if (current_customer == next_customer): recompensa_cambio_cliente = -1\n",
        "      else: recompensa_cambio_cliente = 1\n",
        "\n",
        "      if (action[0] == 0): recompensa_entrega = -1\n",
        "      elif (action[0] == 1): recompensa_entrega = 1\n",
        "      else: recompensa_entrega = 2\n",
        "      \n",
        "      #si el evento era 0 o 5 la recompensa debe ser sobre la eleccion del proximo cliente\n",
        "      if (self.old_event == 5 or self.old_event == 0): recompensa_entrega = 0\n",
        "      #si el evento era 4 la recompensa debe ser sobre la entrega\n",
        "      elif (self.old_event == 4): recompensa_cambio_cliente = 0\n",
        "\n",
        "      recompensa_intermedios = (recompensa_cambio_cliente*10) + (recompensa_entrega*5)\n",
        "\n",
        "      return ts.TimeStep(step_type= ts.StepType.MID, reward=np.array(recompensa_intermedios,dtype=np.float32),  discount=np.array(0.0,dtype=np.float32), observation=np.array(self._state, dtype=np.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Got0hS9auyTC"
      },
      "source": [
        "____________________\n",
        "# **Agente PPO**\n",
        "\n",
        "Implementación de un Agente PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5B-E9rovJLv"
      },
      "source": [
        "## Importación de librerías necesarias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yimeWLImaC0p"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-agents[reverb]\n",
        "#!pip install reverb\n",
        "#!pip install -force-reinstall tf-agents[reverb]\n",
        "#!pip install dm-reverb[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unU5J-PCuy3N"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.ddpg import critic_rnn_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.networks import actor_distribution_rnn_network\n",
        "from tf_agents.networks import nest_map\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from tf_agents.agents.ppo import ppo_clip_agent\n",
        "from tf_agents.agents.ppo import ppo_agent\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.environments import parallel_py_environment\n",
        "from tf_agents.environments import suite_mujoco\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.networks import categorical_projection_network\n",
        "from tf_agents.networks import actor_distribution_rnn_network\n",
        "from tf_agents.networks import value_network\n",
        "from tf_agents.networks import value_rnn_network\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import episodic_replay_buffer\n",
        "\n",
        "from tf_agents.system import system_multiprocessing as multiprocessing\n",
        "from tf_agents.utils import common\n",
        "\n",
        "import tf_agents.networks\n",
        "\n",
        "from tf_agents.agents.ppo import ppo_actor_network\n",
        "from tf_agents.agents.ppo import ppo_clip_agent\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import ppo_learner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja7mrOWD0jcM"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPP6xj3kopAP"
      },
      "outputs": [],
      "source": [
        "batch_size = 1 # @param {type:\"integer\"}\n",
        "n_step_update = 2  # @param {type:\"integer\"}\n",
        "\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "policy_save_interval = 500 # @param {type:\"integer\"}\n",
        "####\n",
        "\n",
        "#\n",
        "estimated_time_VRPenvironment = 130\n",
        "\n",
        "# Root directory for writing logs/summaries/checkpoints.\n",
        "root_dir = os.getenv('TEST_UNDECLARED_OUTPUTS_DIR')\n",
        "\n",
        "# Number of environments to run in parallel\n",
        "num_parallel_environments = 30\n",
        "\n",
        "# Params for simple nets\n",
        "actor_fc_layers=(8, 26)\n",
        "value_fc_layers=(8, 20)\n",
        "\n",
        "# Params for value RNN net\n",
        "v_input_fc_layer_params=(8,)\n",
        "v_lstm_size=(16,)\n",
        "v_output_fc_layer_params=(4,)\n",
        "\n",
        "# Params for actor RNN net\n",
        "a_input_fc_layer_params=(8,)\n",
        "a_lstm_size=(26,)\n",
        "a_output_fc_layer_params=(13,)\n",
        "\n",
        "#replay_buffer_capacity=1001  # Per-environment\n",
        "\n",
        "# Params for train\n",
        "num_epochs=50\n",
        "learning_rate=1e-4\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# Params for summaries and logging\n",
        "train_checkpoint_interval=500\n",
        "policy_checkpoint_interval=500\n",
        "\n",
        "\n",
        "summary_interval=50\n",
        "summaries_flush_secs=1\n",
        "use_tf_functions=True\n",
        "debug_summaries=False\n",
        "summarize_grads_and_vars=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBnRuIw02fXw"
      },
      "source": [
        "## Entorno (Environment)\n",
        "\n",
        "Los entornos en la RL representan la tarea o el problema que estamos tratando de resolver.\n",
        "\n",
        "En este caso vamos a instanciar el entorno utilizando la clase creada TSPEnv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yeaJETb2f3K",
        "outputId": "a450a9b0-bbdf-4336-d65b-184901a06523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': array(1., dtype=float32),\n",
              " 'observation': array([  0.,   0., 410.,   0.], dtype=float32),\n",
              " 'reward': array(0., dtype=float32),\n",
              " 'step_type': array(0, dtype=int32)})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "env = TSPEnv(df_planificacion,130)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZA4bKYj4ge2"
      },
      "source": [
        "Este entorno implementa un problema VRP de entrega de mercadería. \n",
        "El objetivo es que el agente entrene una política que controlará el vehículo para que visite a los clientes y entregue la mercadería, buscando minimizar el tiempo de recorrido y maximizar la satisfacción de los clentes (cumplir con la demanda). El vehículo tiene una restricción de capacidad y un tiempo esperado de viaje. \n",
        "\n",
        "En cada episodio el vehículo parte del depósito y finaliza el recorrido al regresar al mismo. El retorno será **( ..... completar)** \n",
        "\n",
        "\n",
        "A continuación se puede ver la información que proporciona el entorno como una `observación` que la política utilizará para generar `acciones`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGPyJJ1H4g2b",
        "outputId": "93896e4f-06c7-48ad-a12d-f2406a3fe76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Spec:\n",
            "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=3.4028234663852886e+38)\n",
            "Action Spec:\n",
            "BoundedArraySpec(shape=(2,), dtype=dtype('int32'), name='action', minimum=[0 0], maximum=[2 9])\n"
          ]
        }
      ],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMdu5aJW7FtB"
      },
      "source": [
        "Descripción de la observación y de las acciones permitidas............\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83H4LI2C7NSL"
      },
      "source": [
        "Se crean dos entornos: uno para recopilar datos durante el entrenamiento y otro para realizar la evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJZr7anM7NqJ",
        "outputId": "bd59feb4-ef40-4181-bd37-4a8e5c58f921"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': array(1., dtype=float32),\n",
              " 'observation': array([  0.,   0., 410.,   0.], dtype=float32),\n",
              " 'reward': array(0., dtype=float32),\n",
              " 'step_type': array(0, dtype=int32)})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "collect_env = TSPEnv(df_planificacion,130)\n",
        "collect_env.reset()\n",
        "eval_env = TSPEnv(df_planificacion,130)\n",
        "eval_env.reset()\n",
        "train_env = TSPEnv(df_planificacion,130)\n",
        "train_env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS5R6OAPtALk"
      },
      "outputs": [],
      "source": [
        "train_TFenv = tf_py_environment.TFPyEnvironment(train_env)\n",
        "collect_TFenv = tf_py_environment.TFPyEnvironment(collect_env)\n",
        "eval_TFenv = tf_py_environment.TFPyEnvironment(eval_env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-z2yF66FR9"
      },
      "source": [
        "## Distribution Strategy\n",
        "We use the DistributionStrategy API to enable running the train step computation across multiple devices such as multiple GPUs or TPUs using data parallelism. The train step:\n",
        "* Receives a batch of training data\n",
        "* Splits it across the devices\n",
        "* Computes the forward step\n",
        "* Aggregates and computes the MEAN of the loss\n",
        "* Computes the backward step and performs a gradient variable update\n",
        "\n",
        "With TF-Agents Learner API and DistributionStrategy API it is quite easy to switch between running the train step on GPUs (using MirroredStrategy) to TPUs (using TPUStrategy) without changing any of the training logic below.\n",
        "\n",
        "\n",
        "### Enabling the GPU\n",
        "If you want to try running on a GPU, you'll first need to enable GPUs for the notebook:\n",
        "\n",
        "* Navigate to Edit→Notebook Settings\n",
        "* Select GPU from the Hardware Accelerator drop-down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc5BvLlEDACQ"
      },
      "source": [
        "### Picking a strategy\n",
        "Use `strategy_utils` to generate a strategy. Under the hood, passing the parameter:\n",
        "* `use_gpu = False` returns `tf.distribute.get_strategy()`, which uses CPU\n",
        "* `use_gpu = True` returns `tf.distribute.MirroredStrategy()`, which uses all GPUs that are visible to TensorFlow on one machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH9Burb3DPm6",
        "outputId": "187b61dd-09ad-4f16-a12b-fc40f74e74f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        }
      ],
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMn5FTs5kHvt"
      },
      "source": [
        "All variables and Agents need to be created under `strategy.scope()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr7W4ZptEn7r"
      },
      "source": [
        "## Agent\n",
        "\n",
        "To create an SAC Agent, we first need to create the networks that it will train. SAC is an actor-critic agent, so we will need two networks.\n",
        "\n",
        "The critic will give us value estimates for `Q(s,a)`. That is, it will recieve as input an observation and an action, and it will give us an estimate of how good that action was for the given state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR4ewP2HbssL"
      },
      "outputs": [],
      "source": [
        "from tf_agents.networks import utils as network_utils\n",
        "\n",
        "class DummyValueNet(network.Network):\n",
        "\n",
        "  def __init__(self,\n",
        "               observation_spec,\n",
        "               preprocessing_layers=None,\n",
        "               name=None,\n",
        "               outer_rank=1):\n",
        "    super(DummyValueNet, self).__init__(observation_spec, (), 'DummyValueNet')\n",
        "    self._outer_rank = outer_rank\n",
        "    self._dummy_layers = (preprocessing_layers or []) + [\n",
        "        tf.keras.layers.Dense(\n",
        "            1,\n",
        "            kernel_initializer=tf.constant_initializer([6, 1, 6, 1, 6, 1]),\n",
        "            bias_initializer=tf.constant_initializer([5]))\n",
        "    ]\n",
        "\n",
        "  def call(self, inputs, step_type=None, network_state=()):\n",
        "    del step_type\n",
        "    print('inputs: ', inputs)\n",
        "    hidden_state = tf.cast(tf.nest.flatten(inputs), tf.float32)[0]\n",
        "    batch_squash = network_utils.BatchSquash(self._outer_rank)\n",
        "    hidden_state = batch_squash.flatten(hidden_state)\n",
        "    for layer in self._dummy_layers:\n",
        "      hidden_state = layer(hidden_state)\n",
        "    #value_pred = batch_squash.unflatten(hidden_state)\n",
        "    value_pred = tf.squeeze(batch_squash.unflatten(hidden_state), axis=-1)\n",
        "    print('value pred: ', value_pred)\n",
        "    return value_pred, network_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPeklh_LDfB0"
      },
      "outputs": [],
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "#observation_spec = tensor_spec.TensorSpec([6], tf.float32)\n",
        "#time_step_spec = ts.time_step_spec(observation_spec)\n",
        "#action_spec = tensor_spec.BoundedTensorSpec([3], tf.int32, 0, 10)\n",
        "# action_spec = array_spec.BoundedArraySpec(shape=(3,), dtype=np.int32, minimum=0, name='action')\n",
        "\n",
        "with strategy.scope():\n",
        "  value_net = value_network.ValueNetwork(\n",
        "          observation_spec,\n",
        "          fc_layer_params=value_fc_layers,\n",
        "          dropout_layer_params=None,\n",
        "               activation_fn=tf.keras.activations.relu,\n",
        "               kernel_initializer=None,\n",
        "               batch_squash=True,\n",
        "               dtype=tf.float32,\n",
        "               name='ValueNetwork')\n",
        "\n",
        "  value_rnn_net = value_rnn_network.ValueRnnNetwork(\n",
        "                  observation_spec,\n",
        "                  input_fc_layer_params = v_input_fc_layer_params,\n",
        "                  lstm_size = v_lstm_size,\n",
        "                  output_fc_layer_params = v_output_fc_layer_params,\n",
        "                  activation_fn=tf.keras.activations.relu,\n",
        "                  dtype=tf.float32,\n",
        "                  name='ValueRNNNetwork')\n",
        "\n",
        "#with strategy.scope():\n",
        "#  value_net = DummyValueNet(observation_spec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WV6Q1roezxF",
        "outputId": "24c07a65-b13a-4edc-bc8e-9f3ba0468c9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(3.4028235e+38, dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "observation_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJCNpvILam14",
        "outputId": "30f0c707-3bbb-4c8f-d4de-e159fd1abcc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.networks.value_network.ValueNetwork at 0x7ff4707a9190>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "value_net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur-EeyvJE2E-"
      },
      "source": [
        "We will use this critic to train an `actor` network which will allow us to generate actions given an observation.\n",
        "\n",
        "The `ActorNetwork` will predict parameters for a tanh-squashed [MultivariateNormalDiag](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) distribution. This distribution will then be sampled, conditioned on the current observation, whenever we need to generate actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWNihO2oT_no",
        "outputId": "322487ed-c13a-4bb0-df0a-48639456bef0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 9], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "np.sum(9)\n",
        "np.sum(action_spec.maximum)\n",
        "action_spec.maximum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTqUiRN5L3Ii"
      },
      "outputs": [],
      "source": [
        "import gin\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.networks import utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.specs import distribution_spec\n",
        "\n",
        "class MultiCategoricalDistributionBlock(tfp.distributions.Blockwise):\n",
        "\n",
        "    def __init__(self, logits, categories_shape):\n",
        "        self.categories_shape = categories_shape\n",
        "        distribs = self._create_distrib(logits)\n",
        "        super().__init__(distributions = distribs)\n",
        "        self._add_logits_to_params(logits)\n",
        "\n",
        "    def _create_distrib(self, logits):\n",
        "        logits = tf.split(logits, self.categories_shape, -1)\n",
        "        distribs = [tfp.distributions.Categorical(logits = logits_split) for \n",
        "                        logits_split in logits]\n",
        "        return distribs\n",
        "\n",
        "    def _mode(self):\n",
        "        return self._flatten_and_concat_event(\n",
        "            self._distribution.mode()\n",
        "        )\n",
        "    \n",
        "    def _add_logits_to_params(self, logits):\n",
        "        self._parameters['logits'] = logits\n",
        "\n",
        "@gin.configurable\n",
        "class MultiCategoricalProjectionNetwork(network.DistributionNetwork):\n",
        "    \"\"\"Generates a set of tfp.distribution.Categorical by predicting logits\"\"\"\n",
        "    def __init__(self, \n",
        "                 sample_spec, \n",
        "                 logits_init_output_factor=0.1,\n",
        "                 name='MultiCategoricalProjectionNetwork'):\n",
        "    \n",
        "        \"\"\"Creates an instance of MultiCategoricalProjectionNetwork\n",
        "        Args:\n",
        "          sample_spec: A `tensor_spec.BoundedTensorSpec` detailing the shape\n",
        "            dtypes of samples pulled from the output distribution.\n",
        "          logits_init_output_factor: Output factor for initializing kernal \n",
        "            logits weights.\n",
        "          name: A string representing the name of the network.\n",
        "        \"\"\"\n",
        "\n",
        "        self.categories_shape = sample_spec.maximum+1\n",
        "        self.n_unique_categories = np.sum(self.categories_shape)\n",
        "\n",
        "        output_spec = self._output_distribution_spec([self.n_unique_categories], sample_spec, name)\n",
        "\n",
        "        super(MultiCategoricalProjectionNetwork, self).__init__(\n",
        "            input_tensor_spec=None,\n",
        "            state_spec = (),\n",
        "            output_spec=output_spec,\n",
        "            name=name\n",
        "        )\n",
        "\n",
        "        if not tensor_spec.is_bounded(sample_spec):\n",
        "            raise ValueError(\n",
        "                'sample_spec must be bounded. Got: %s.' % type(sample_spec))\n",
        "\n",
        "        if not tensor_spec.is_discrete(sample_spec):\n",
        "            raise ValueError('sample_spec must be discrete. Got: %s.' % sample_spec)        \n",
        "\n",
        "        self._sample_spec = sample_spec\n",
        "\n",
        "        self._projection_layer = tf.keras.layers.Dense(\n",
        "            self.n_unique_categories,\n",
        "            kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                scale=logits_init_output_factor),\n",
        "            bias_initializer=tf.keras.initializers.Zeros(),\n",
        "            name='logits'\n",
        "            )       \n",
        "\n",
        "    def _output_distribution_spec(self, output_shape, sample_spec, network_name):\n",
        "        input_param_spec = {\n",
        "            'logits':\n",
        "                tensor_spec.TensorSpec(\n",
        "                    shape=output_shape,\n",
        "                    dtype=tf.float32,\n",
        "                    name=network_name + '_logits'\n",
        "                )\n",
        "        }\n",
        "        return distribution_spec.DistributionSpec(\n",
        "            MultiCategoricalDistributionBlock,\n",
        "            input_param_spec,\n",
        "            sample_spec=sample_spec,\n",
        "            categories_shape = self.categories_shape)\n",
        "\n",
        "    def call(self, inputs, outer_rank, training=False, mask=None):\n",
        "        #masks not implemented yet\n",
        "        \n",
        "        batch_squash = utils.BatchSquash(outer_rank)\n",
        "        inputs = batch_squash.flatten(inputs)\n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "\n",
        "        logits = self._projection_layer(inputs, training=training)\n",
        "        logits = tf.reshape(logits, [-1] + [self.n_unique_categories])\n",
        "        logits = batch_squash.unflatten(logits)\n",
        "\n",
        "        return self.output_spec.build_distribution(logits= logits), ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibyGBFj8S6sB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _get_inputs(batch_size, num_input_dims):\n",
        "  return tf.random.uniform([batch_size, num_input_dims])\n",
        "\n",
        "output_spec = tensor_spec.BoundedTensorSpec([2,], tf.int32, 0, maximum=[2, 9])\n",
        "categorical_network = MultiCategoricalProjectionNetwork(output_spec)\n",
        "\n",
        "inputs = _get_inputs(batch_size=2, num_input_dims=9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip3byMX9TxQP"
      },
      "outputs": [],
      "source": [
        "def multi_categorical_projection_net(action_spec, logits_init_output_factor=0.1):\n",
        "  return MultiCategoricalProjectionNetwork(action_spec, logits_init_output_factor=logits_init_output_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLmE1zzzE2gj"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      input_tensor_spec=observation_spec,\n",
        "      output_tensor_spec=action_spec,\n",
        "      fc_layer_params=actor_fc_layers,\n",
        "      discrete_projection_net= multi_categorical_projection_net) #(inputs, outer_rank=1)\n",
        "\n",
        "  actor_rnn_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n",
        "      input_tensor_spec=observation_spec,\n",
        "      output_tensor_spec=action_spec, \n",
        "      input_fc_layer_params=a_input_fc_layer_params,\n",
        "      lstm_size=a_lstm_size,\n",
        "      output_fc_layer_params=a_output_fc_layer_params,\n",
        "      activation_fn=tf.keras.activations.relu,\n",
        "      dtype=tf.float32,\n",
        "      discrete_projection_net=multi_categorical_projection_net,\n",
        "      name='ActorDistributionRnnNetwork')\n",
        "               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haeZMw4tFfCw"
      },
      "source": [
        "With these networks at hand we can now instantiate the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNFrd7INxCfZ",
        "outputId": "ba057d09-c19f-42c6-80c8-3d2fb3b967c7",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Only tf.keras.optimizers.Optimiers are well supported, got a non-TF2 optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x7ff478988490>\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = ppo_agent.PPOAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        optimizer,\n",
        "        actor_net = actor_rnn_net, #actor_net=actor_net,\n",
        "        value_net = value_rnn_net, #value_net=value_net,\n",
        "        entropy_regularization=0.0,\n",
        "        importance_ratio_clipping=0.0,\n",
        "        normalize_observations=False,\n",
        "        normalize_rewards=False,\n",
        "        use_gae=True,\n",
        "        num_epochs=num_epochs,\n",
        "        initial_adaptive_kl_beta = 0.0,\n",
        "        kl_cutoff_factor = 0.0,\n",
        "        compute_value_and_advantage_in_train = True, #ver\n",
        "        debug_summaries=debug_summaries,\n",
        "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
        "        train_step_counter=train_step)\n",
        "  \n",
        "  tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t92jXgW3RpOi",
        "outputId": "72b0f74f-b5b0-4485-b0ed-51e248039833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistributionSpec(builder=<class '__main__.MultiCategoricalDistributionBlock'>, input_params_spec={'logits': TensorSpec(shape=(13,), dtype=tf.float32, name='MultiCategoricalProjectionNetwork_logits')}, sample_spec=BoundedTensorSpec(shape=(2,), dtype=tf.int32, name='action', minimum=array([0, 0], dtype=int32), maximum=array([2, 9], dtype=int32)))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "actor_net.output_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EX-Y616ADUh"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes. We can compute the average return metric as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9gPUYpX28tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3436c194-39c0-4eb1-c15a-1331b6966bdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137.2637071609497"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "num_eval_episodes = 1\n",
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return\n",
        "\n",
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  train_env.time_step_spec(), train_env.action_spec())\n",
        "\n",
        "#random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "#                                                train_env.action_spec())\n",
        "\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "\n",
        "# Please also see the metrics module for standard implementations of different\n",
        "# metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5jqBCEfXubH"
      },
      "source": [
        "_______________________\n",
        "\n",
        "\n",
        "# **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtyQnCGcPtMA"
      },
      "outputs": [],
      "source": [
        "#policy_state_base = [ tf.constant([[1., 2., 3.,4.,5.,6.,7.,8.,9.,10.,11.,12.,13.]]), tf.constant([0])]\n",
        "\n",
        "policy_state_base = tf.constant([[1., 2., 3.,4.,5.,6.,7.,8.,9.,10.,11.,12.,13.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7_iouBJXu2F"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import q_rnn_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import episodic_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "#from SimpleMemEnv import Environment as Env\n",
        "from tf_agents.environments import wrappers\n",
        "\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=20, policy_state=()):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      if policy_state:\n",
        "          action_step = policy.action(time_step, policy_state)\n",
        "          policy_state = action_step.state\n",
        "      else:\n",
        "          action_step = policy.action(time_step)\n",
        "\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "def collect_step(environment, policy, buffer, id, policy_state):\n",
        "  time_step = environment.current_time_step()\n",
        "  if policy_state: #len(policy_state.numpy()[0]): #len(policy_state[0].numpy()[0]): #policy_state: # \n",
        "      action_step = policy.action(time_step, policy_state)\n",
        "      #print('info policy: ', action_step.state)\n",
        "      if action_step.info == (): \n",
        "        action_step = action_step.replace(info=policy_state)\n",
        "        #print('info policyyyy: ', action_step.info)\n",
        "      if action_step.state != ():  \n",
        "        policy_state = action_step.state\n",
        "  else:\n",
        "      action_step = policy.action(time_step)\n",
        "\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "  #print(traj) ###\n",
        "\n",
        "  id_tensor = tf.constant(id, dtype=tf.int64)\n",
        "  buffer.add_batch(traj, id_tensor)\n",
        "  if time_step.is_last():\n",
        "      id[0] += 1\n",
        "\n",
        "  return policy_state\n",
        "\n",
        "def collect_data(env, policy, buffer, steps, id, policy_state = ()):#policy_state_base):\n",
        "  #if type(policy_state) == tuple: policy_state = policy_state_base\n",
        "  for _ in range(steps):\n",
        "    policy_state = collect_step(env, policy, buffer, id, policy_state)\n",
        "\n",
        "  return policy_state\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9Zk78RE1Tu0"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "num_iterations = 1000\n",
        "collect_steps_per_iteration = 30\n",
        "initial_collect_steps = 1000\n",
        "replay_buffer_capacity = 100000\n",
        "batch_size = 55 \n",
        "\n",
        "log_interval = 50\n",
        "num_eval_episodes = 10\n",
        "eval_interval = 50  \n",
        "\n",
        "\n",
        "#collect_env = TSPEnv(df_planificacion,130)\n",
        "#eval_env = TSPEnv(df_planificacion,130)\n",
        "#train_env = TSPEnv(df_planificacion,130)\n",
        "\n",
        "\n",
        "train_py_env  = TSPEnv(df_planificacion,130)\n",
        "eval_py_env  = TSPEnv(df_planificacion,130)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "train_env.reset()\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "eval_env.reset()\n",
        "    \n",
        "agent = tf_agent\n",
        "agent.initialize()\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "\n",
        "replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n",
        "    data_spec= agent.collect_data_spec, #agent.training_data_spec, \n",
        "    capacity = replay_buffer_capacity,\n",
        "    completed_only = True,\n",
        "    buffer_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uomlkDQaMkZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3d3603-11c2-4805-c69b-a1830aa6522c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_TupleWrapper(Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(2,), dtype=tf.int32, name='action', minimum=array([0, 0], dtype=int32), maximum=array([2, 9], dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
              " 'policy_info': {'dist_params': {'logits': TensorSpec(shape=(13,), dtype=tf.float32, name='MultiCategoricalProjectionNetwork_logits')}},\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "#agent.training_data_spec\n",
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2YbK5MFA0j1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573fca41-8ad7-45b8-e180-955152550711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 0: Average Return = 0\n"
          ]
        }
      ],
      "source": [
        "episode_id = [0]\n",
        "\n",
        "policy_state = agent.policy.get_initial_state(batch_size=train_env.batch_size)\n",
        "collect_policy_state = agent.collect_policy.get_initial_state(batch_size=train_env.batch_size)\n",
        "\n",
        "#collect_data(train_env, random_policy, replay_buffer, initial_collect_steps, episode_id, collect_policy_state) \n",
        "#collect_data(train_env, agent.policy, replay_buffer, initial_collect_steps, episode_id, policy_state)\n",
        "\n",
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=None,\n",
        "    num_steps=None)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = 0\n",
        "returns = [avg_return]\n",
        "\n",
        "print('step = {0}: Average Return = {1}'.format(0, avg_return))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgCvZ9h23uEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c59059-66f1-4ba9-9440-8caad04b7416"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DictWrapper({'actor_network_state': ListWrapper([<tf.Tensor: shape=(1, 26), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(1, 26), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>])})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "collect_policy_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugWPPoe2rNCC"
      },
      "outputs": [],
      "source": [
        "step = 0\n",
        "train_loss = None\n",
        "value_estimation_loss = 0\n",
        "policy_gradient_loss = 0\n",
        "loss = 0\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_policy_state = collect_data(train_env,\n",
        "                                   agent.collect_policy,\n",
        "                                   replay_buffer,\n",
        "                                   collect_steps_per_iteration,\n",
        "                                   episode_id,\n",
        "                                   collect_policy_state)#policy_state)\n",
        "\n",
        "  #print(collect_policy_state)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "\n",
        "  step += 1\n",
        "      \n",
        "  if step % 1 == 0: #(num_iterations/50)==0: #50 == 0:\n",
        "    \n",
        "    # Dataset generates trajectories with shape [Bx2x...]\n",
        "    dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=None,\n",
        "    num_steps=None)\n",
        "    iterator = iter(dataset)\n",
        "    \n",
        "    #experience, unused_info = next(iterator)\n",
        "    #train_loss = agent.train(experience).loss\n",
        "    experience, unused_info = next(iterator)\n",
        "    batched_exp = tf.nest.map_structure(\n",
        "    lambda t: tf.expand_dims(t, axis=0),\n",
        "    experience\n",
        "    )\n",
        "    #print(batched_exp)\n",
        "    train_loss = agent.train(batched_exp)\n",
        "\n",
        "\n",
        "    #replay_buffer.clear()\n",
        "    value_estimation_loss = train_loss.extra.value_estimation_loss.numpy()\n",
        "    policy_gradient_loss = train_loss.extra.policy_gradient_loss.numpy()\n",
        "    loss = train_loss.loss.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1} policy loss = {2} value estimation loss = {3}'.format(step, loss, policy_gradient_loss, value_estimation_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    policy_state = agent.policy.get_initial_state(batch_size=train_env.batch_size)\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes, policy_state)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "  \n",
        "    \n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Experimentos_Agente_VRP_InstanciaHuDi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}